---
title: "Xử lý ngôn ngữ tự nhiên (NLP) với Transformers"
description: "Tìm hiểu kiến trúc Transformer và cách áp dụng vào các bài toán NLP như text classification, NER, và machine translation."
pubDate: 2026-02-03
author: "AI Blog"
category: "NLP"
tags: ["nlp", "transformers", "bert", "gpt", "huggingface"]
image: ""
---

## Transformer Architecture

Kiến trúc **Transformer** được giới thiệu trong paper "Attention Is All You Need" (2017) đã cách mạng hóa lĩnh vực NLP.

## Self-Attention Mechanism

Công thức tính Attention:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

Trong đó:
- $Q$ (Query), $K$ (Key), $V$ (Value) là các ma trận
- $d_k$ là chiều của Key vectors

## Multi-Head Attention

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

với mỗi head:

$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

## Sử dụng Hugging Face Transformers

```python
from transformers import AutoTokenizer, AutoModel
import torch

# Load pre-trained model và tokenizer
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Tokenize text
text = "Natural Language Processing is fascinating!"
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)

# Get embeddings
with torch.no_grad():
    outputs = model(**inputs)
    embeddings = outputs.last_hidden_state

print(f"Embedding shape: {embeddings.shape}")
# Output: torch.Size([1, 9, 768])
```

## Text Classification với BERT

```python
from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset

# Load dataset
dataset = load_dataset("imdb")

# Load model for classification
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
)

# Tokenize
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=512
    )

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Training
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
)

trainer.train()
```

## So sánh các mô hình

| Model | Parameters | Specialty |
|-------|-----------|-----------|
| BERT | 110M - 340M | Bidirectional understanding |
| GPT-2 | 117M - 1.5B | Text generation |
| GPT-3 | 175B | Few-shot learning |
| T5 | 60M - 11B | Text-to-text framework |
| RoBERTa | 125M - 355M | Optimized BERT training |

## Kết luận

Transformers đã trở thành kiến trúc tiêu chuẩn cho NLP và đang mở rộng sang Computer Vision (ViT), Audio (Whisper), và nhiều lĩnh vực khác!
